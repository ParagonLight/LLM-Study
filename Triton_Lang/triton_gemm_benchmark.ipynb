{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotune\n",
    "\n",
    "ä¹‹å‰æˆ‘ä»¬ç®€å•éªŒè¯äº†æˆ‘ä»¬è‡ªå®šä¹‰ GEMM kernel çš„æ­£ç¡®æ€§ï¼Œè¿˜å‰©ä¸‹ä¸€ä¸ªå·¥ä½œæ²¡åšï¼Œä¹Ÿå°±æ˜¯è¶…å‚æ•°çš„ç¡®å®šï¼Œå³ `BLOCK_SIZE_M` ç­‰å€¼åº”è¯¥æ€ä¹ˆå–\n",
    "\n",
    "è¿™é‡Œåªéœ€è¦ä½¿ç”¨ `@triton.autotune` å³å¯ï¼ŒTriton çš„ç¼–è¯‘å™¨ä¼šåœ¨æˆ‘ä»¬çš„ config åˆ—è¡¨ä¸­æ‰¾åˆ°æœ€ä½³çš„é…ç½®ï¼Œè¿™é‡Œéœ€è¦è¯´æ˜ä¸¤ä¸ªå‚æ•°ï¼š\n",
    "\n",
    "- `num_stages`ï¼šå®˜æ–¹æ–‡æ¡£ä¸­è¯´è¿™ä¸ªå‚æ•°è¡¨ç¤º \"the number of stages that the compiler should use when software-pipelining loops\"ï¼Œæˆ‘å°è¯•æŸ¥è¿‡ç›¸å…³å†…å®¹ï¼Œä¸è¿‡è§£é‡Šåœ°ä¸ç®—å¾ˆæ¸…æ¥šï¼Œå¤§è‡´å¯ä»¥è®¤ä¸ºæ˜¯ kernel ä¸­å¾ªç¯ç»“æ„çš„å¹¶è¡Œåº¦ï¼Œè¿™æ˜¯é€šè¿‡ Triton ç¼–è¯‘å™¨è°ƒæ•´å¾ªç¯çš„å®é™…ç»“æ„å®ç°çš„\n",
    "- `num_warps`ï¼šå½“å‰ block å¸Œæœ›ä½¿ç”¨çš„ warp çš„æ•°é‡ï¼Œä¸€ä¸ª warp å¯¹åº” 32 ä¸ªçº¿ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    A_ptr, B_ptr, C_ptr, \n",
    "    M, N, K, \n",
    "    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n",
    "\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        accumulator += tl.dot(a, b)\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = C_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    c = accumulator.to(tl.float16)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def grid(META):\n",
    "    return (triton.cdiv(META['M'], META['BLOCK_SIZE_M']), triton.cdiv(META['N'], META['BLOCK_SIZE_N']))\n",
    "\n",
    "def matmul(A, B):\n",
    "    assert A.shape[1] == B.shape[0]\n",
    "    M, K = A.shape\n",
    "    K, N = B.shape\n",
    "    C = torch.empty((M, N), device='cuda', dtype=torch.float16)\n",
    "    matmul_kernel[grid](\n",
    "        A, B, C, \n",
    "        M, N, K, \n",
    "        A.stride(0), A.stride(1), B.stride(0), B.stride(1), C.stride(0), C.stride(1),)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "è®©æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹æˆ‘ä»¬çš„ kernel çš„æ•ˆç‡æ€ä¹ˆæ · ğŸ‘€\n",
    "\n",
    "å¦‚æœå°è¯•ç”¨ä¸åŒçš„æ˜¾å¡ï¼Œä¼šå‘ç°åœ¨ TITAN ä¸Š Triton çš„æ•ˆç‡è¿œä½äº cuBLASï¼Œä½†æ˜¯åœ¨ A6000 ä¸Šä¸¤è€…å‡ ä¹å·®ä¸å¤šï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­æ˜¯ Triton ç¼–è¯‘å™¨å¯¹äºæ—§çš„æ¶æ„ä¼˜åŒ–æœ‰é™å¯¼è‡´çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=['cublas', 'triton'],\n",
    "        # Label name for the lines\n",
    "        line_names=[\"cuBLAS\", \"Triton\"],\n",
    "        # Line styles\n",
    "        styles=[('green', '-'), ('red', '-')],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    ))\n",
    "def benchmark(M, N, K, provider):\n",
    "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    # pytorch ä½¿ç”¨çš„ cuBLAS å®ç°\n",
    "    if provider == 'cublas':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    # æˆ‘ä»¬è‡ªå·±å®šä¹‰çš„ kernel\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    # ms è¡¨ç¤ºä¸€æ¬¡è°ƒç”¨çš„å¹³å‡æ—¶é—´\n",
    "    # è®¡ç®— TFLOPS æ—¶ï¼š\n",
    "    #  - 2 è¡¨ç¤ºæˆ‘ä»¬çš„å•ä½æ˜¯ torch.float16ï¼Œå› æ­¤æ˜¯ 2 å­—èŠ‚\n",
    "    #  - 1e-12 è½¬æ¢æˆ T\n",
    "    #  - 1e-12 è½¬æ¢æˆç§’\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
