{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotune\n",
    "\n",
    "之前我们简单验证了我们自定义 GEMM kernel 的正确性，还剩下一个工作没做，也就是超参数的确定，即 `BLOCK_SIZE_M` 等值应该怎么取\n",
    "\n",
    "这里只需要使用 `@triton.autotune` 即可，Triton 的编译器会在我们的 config 列表中找到最佳的配置，这里需要说明两个参数：\n",
    "\n",
    "- `num_stages`：官方文档中说这个参数表示 \"the number of stages that the compiler should use when software-pipelining loops\"，我尝试查过相关内容，不过解释地不算很清楚，大致可以认为是 kernel 中循环结构的并行度，这是通过 Triton 编译器调整循环的实际结构实现的\n",
    "- `num_warps`：当前 block 希望使用的 warp 的数量，一个 warp 对应 32 个线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    A_ptr, B_ptr, C_ptr, \n",
    "    M, N, K, \n",
    "    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n",
    "\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        accumulator += tl.dot(a, b)\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = C_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    c = accumulator.to(tl.float16)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def grid(META):\n",
    "    return (triton.cdiv(META['M'], META['BLOCK_SIZE_M']), triton.cdiv(META['N'], META['BLOCK_SIZE_N']))\n",
    "\n",
    "def matmul(A, B):\n",
    "    assert A.shape[1] == B.shape[0]\n",
    "    M, K = A.shape\n",
    "    K, N = B.shape\n",
    "    C = torch.empty((M, N), device='cuda', dtype=torch.float16)\n",
    "    matmul_kernel[grid](\n",
    "        A, B, C, \n",
    "        M, N, K, \n",
    "        A.stride(0), A.stride(1), B.stride(0), B.stride(1), C.stride(0), C.stride(1),)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "让我们来测试一下我们的 kernel 的效率怎么样 👀\n",
    "\n",
    "如果尝试用不同的显卡，会发现在 TITAN 上 Triton 的效率远低于 cuBLAS，但是在 A6000 上两者几乎差不多，我们可以推断是 Triton 编译器对于旧的架构优化有限导致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=['cublas', 'triton'],\n",
    "        # Label name for the lines\n",
    "        line_names=[\"cuBLAS\", \"Triton\"],\n",
    "        # Line styles\n",
    "        styles=[('green', '-'), ('red', '-')],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    ))\n",
    "def benchmark(M, N, K, provider):\n",
    "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    # pytorch 使用的 cuBLAS 实现\n",
    "    if provider == 'cublas':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    # 我们自己定义的 kernel\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    # ms 表示一次调用的平均时间\n",
    "    # 计算 TFLOPS 时：\n",
    "    #  - 2 表示我们的单位是 torch.float16，因此是 2 字节\n",
    "    #  - 1e-12 转换成 T\n",
    "    #  - 1e-12 转换成秒\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
