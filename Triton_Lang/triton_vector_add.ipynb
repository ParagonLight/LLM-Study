{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `@triton.jit` 可以理解为告诉编译器这是一个 Triton 的 kernel 函数\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr,  # 指向向量 x 第一个元素的指针\n",
    "               y_ptr,  # 指向向量 y 第一个元素的指针\n",
    "               output_ptr,  \n",
    "               n_elements,  # Size of the vector.\n",
    "               BLOCK_SIZE: tl.constexpr,  # 可以认为是当前 kernel 的超参数\n",
    "               ):\n",
    "    # 当前 block 的 index，暂时跳过，后面会具体说明，可以暂时理解成我们平时写的 for 循环的索引（变量 i）\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # 当前 block 第一个元素的索引\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    # 当前 block 中所有元素的索引\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    # `tl.load` 会从 DRAM 中加载出向量 x、y 对应的 block 中的所有元素\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    # 真正的计算只有这一行\n",
    "    output = x + y\n",
    "    # 从 SM 中把计算得到的结果写回 DRAM\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel 的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel()\n",
    "\n",
    "    # `triton.cdiv` 的功能是向上取整\n",
    "    # `grid` 的功能是计算出当前的计算需要划分出多少个 block\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "\n",
    "    # `add_kernel`` 的调用\n",
    "    # `grid` 和 `add_kernel` 共享参数\n",
    "    # meta 表示 `add_kernel`` 的参数，所以 meta['BLOCK_SIZE'] 相当于取出了 BLOCK_SIZE，这里就是512\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=512)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单元测试\n",
    "\n",
    "让我们来测试一下我们的 kernel 是否正确 👀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3713, 1.3076, 0.4940,  ..., 1.1147, 1.1906, 1.5746], device='cuda:0')\n",
      "tensor([1.3713, 1.3076, 0.4940,  ..., 1.1147, 1.1906, 1.5746], device='cuda:0')\n",
      "The maximum difference between torch and triton is 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "size = 98432\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "\n",
    "# pytorch 实现\n",
    "output_torch = x + y\n",
    "# Triton 实现\n",
    "output_triton = add(x, y)\n",
    "\n",
    "# 结果打印\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
